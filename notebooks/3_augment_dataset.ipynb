{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from datasets import Dataset\n",
    "import time\n",
    "\n",
    "# --- 1. Translation pipelines for backtranslation ---\n",
    "# English <-> Spanish\n",
    "en2es = pipeline('translation_en_to_es', model='Helsinki-NLP/opus-mt-en-es', device=0)\n",
    "es2en = pipeline('translation_es_to_en', model='Helsinki-NLP/opus-mt-es-en', device=0)\n",
    "# English <-> German\n",
    "en2de = pipeline('translation_en_to_de', model='Helsinki-NLP/opus-mt-en-de', device=0)\n",
    "de2en = pipeline('translation_de_to_en', model='Helsinki-NLP/opus-mt-de-en', device=0)\n",
    "# English <-> French\n",
    "en2fr = pipeline('translation_en_to_fr', model='Helsinki-NLP/opus-mt-en-fr', device=0)\n",
    "fr2en = pipeline('translation_fr_to_en', model='Helsinki-NLP/opus-mt-fr-en', device=0)\n",
    "\n",
    "def safe_translate(pipe, text):\n",
    "    # Sometimes huggingface pipelines hit rate limits or errors\n",
    "    for _ in range(3):\n",
    "        try:\n",
    "            return pipe(text)[0]['translation_text']\n",
    "        except Exception as e:\n",
    "            print(\"Retrying due to:\", e)\n",
    "            time.sleep(1)\n",
    "    return text  # fallback\n",
    "\n",
    "def backtranslate(text, lang):\n",
    "    if lang == 0:  # Assuming 'en' is represented by 0\n",
    "        es = safe_translate(en2es, text)\n",
    "        bt_en = safe_translate(es2en, es)\n",
    "        de = safe_translate(en2de, text)\n",
    "        bt_en_2 = safe_translate(de2en, de)\n",
    "        return [bt_en, bt_en_2]\n",
    "    elif lang == 3:  # Assuming 'es' is represented by 3\n",
    "        en = safe_translate(es2en, text)\n",
    "        bt_es = safe_translate(en2es, en)\n",
    "        return [bt_es]\n",
    "    elif lang == 2:  # Assuming 'de' is represented by 2\n",
    "        en = safe_translate(de2en, text)\n",
    "        bt_de = safe_translate(en2de, en)\n",
    "        return [bt_de]\n",
    "    elif lang == 1:  # Assuming 'fr' is represented by 1\n",
    "        en = safe_translate(fr2en, text)\n",
    "        bt_fr = safe_translate(en2fr, en)\n",
    "        return [bt_fr]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# --- 2. Paraphrase for English (optional, since EN has good models) ---\n",
    "paraphrase_tokenizer = AutoTokenizer.from_pretrained(\"Vamsi/T5_Paraphrase_Paws\")\n",
    "paraphrase_model = AutoModelForSeq2SeqLM.from_pretrained(\"Vamsi/T5_Paraphrase_Paws\")\n",
    "\n",
    "def paraphrase_en(text, num_return_sequences=1):\n",
    "    input_text = f\"paraphrase: {text} </s>\"\n",
    "    features = paraphrase_tokenizer([input_text], return_tensors='pt', truncation=True)\n",
    "    output = paraphrase_model.generate(\n",
    "        **features,\n",
    "        max_length=64,\n",
    "        num_beams=10,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        temperature=1.5\n",
    "    )\n",
    "    return [paraphrase_tokenizer.decode(o, skip_special_tokens=True) for o in output]\n",
    "\n",
    "# --- 3. Apply pipeline to Hugging Face Dataset ---\n",
    "\n",
    "def augment_row(row):\n",
    "    orig_text = row['sentence']\n",
    "    lang = row['lang']  # Should be 'en', 'es', or 'de'\n",
    "    augmented = []\n",
    "    # Backtranslation\n",
    "    augmented += backtranslate(orig_text, lang)\n",
    "    # # Paraphrasing (only for English)\n",
    "    # if lang == 0:  # Assuming 'en' is represented by 0\n",
    "    #     augmented += paraphrase_en(orig_text, num_return_sequences=2)\n",
    "    # Keep original\n",
    "    return {'augmented_sentences': [orig_text] + augmented, 'labels': [row['labels']] * (1 + len(augmented)), 'lang': [lang] * (1 + len(augmented))}\n",
    "\n",
    "# Assuming your Hugging Face dataset is loaded as \"dataset\"\n",
    "# Let's say we're augmenting only the minority classes:\n",
    "minority_labels = [1, 2]  # Adjust as needed\n",
    "\n",
    "def augment_dataset(dataset):\n",
    "    new_data = {'sentence': [], 'labels': [], 'lang': []}\n",
    "    for row in dataset:\n",
    "        if row['labels'] in minority_labels:\n",
    "            aug = augment_row(row)\n",
    "            for s, l, lang in zip(aug['augmented_sentences'], aug['labels'], aug['lang']):\n",
    "                new_data['sentence'].append(s)\n",
    "                new_data['labels'].append(l)\n",
    "                new_data['lang'].append(lang)\n",
    "        else:\n",
    "            # Add original only for majority class\n",
    "            new_data['sentence'].append(row['sentence'])\n",
    "            new_data['labels'].append(row['labels'])\n",
    "            new_data['lang'].append(row['lang'])\n",
    "\n",
    "    # Note: This assumes that the original dataset is in the same format as the augmented one\n",
    "    return Dataset.from_dict(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# dataset = load_dataset(\"nojedag/financial-tweets-sentiment-multilingual\")\n",
    "dataset = load_dataset(\"nojedag/financial_phrasebank_multilingual\")\n",
    "\n",
    "# get a sample of 1000 elements\n",
    "dataset[\"train\"] = dataset['train'].shuffle(seed=42).select(range(80))\n",
    "dataset[\"test\"] = dataset['test'].shuffle(seed=42).select(range(20))\n",
    "\n",
    "def prepare_dataset(dataset):\n",
    "    dataset = dataset.rename_column(\"sentiment\", \"labels\")\n",
    "    return dataset\n",
    "\n",
    "dataset = prepare_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[32m      3\u001b[39m dataset = load_dataset(\u001b[33m\"\u001b[39m\u001b[33mnojedag/financial_phrasebank_multilingual\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m dataset[\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43maugment_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m dataset[\u001b[33m'\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m'\u001b[39m] = augment_dataset(dataset[\u001b[33m'\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 85\u001b[39m, in \u001b[36maugment_dataset\u001b[39m\u001b[34m(dataset)\u001b[39m\n\u001b[32m     83\u001b[39m new_data = {\u001b[33m'\u001b[39m\u001b[33msentence\u001b[39m\u001b[33m'\u001b[39m: [], \u001b[33m'\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m'\u001b[39m: [], \u001b[33m'\u001b[39m\u001b[33mlang\u001b[39m\u001b[33m'\u001b[39m: []}\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlabels\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;129;01min\u001b[39;00m minority_labels:\n\u001b[32m     86\u001b[39m         aug = augment_row(row)\n\u001b[32m     87\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m s, l, lang \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(aug[\u001b[33m'\u001b[39m\u001b[33maugmented_sentences\u001b[39m\u001b[33m'\u001b[39m], aug[\u001b[33m'\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m'\u001b[39m], aug[\u001b[33m'\u001b[39m\u001b[33mlang\u001b[39m\u001b[33m'\u001b[39m]):\n",
      "\u001b[31mKeyError\u001b[39m: 'labels'"
     ]
    }
   ],
   "source": [
    "dataset['train'] = augment_dataset(dataset['train'])\n",
    "dataset['test'] = augment_dataset(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push the balanced dataset to Hugging Face Hub\n",
    "dataset.push_to_hub(\"nojedag/financial_phrasebank_multilingual_augmented\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
