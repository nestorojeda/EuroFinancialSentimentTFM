{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Add the parent directory to the path so Python can find the toolbox package\n",
        "module_path = os.path.abspath(os.path.join('..'))\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)\n",
        "    print(f\"Added {module_path} to sys.path\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Check CUDA availability\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
        "    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA current device: {torch.cuda.current_device()}\")\n",
        "    # Set deterministic behavior for reproducibility\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "else:\n",
        "    print(\"WARNING: CUDA is not available. Training will be much slower on CPU.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Create weight tensor and move it to the correct device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name = 'finbert-finetuned-financial-news-sentiment-analysis-european'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('ProsusAI/finbert', num_labels=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ── label map ────────────────────────────────────────────────────────────────\n",
        "# Keep this next to your tokenizer/model init so everything shares the same map\n",
        "label2id = {\"neutral\": 0, \"positive\": 1, \"negative\": 2}\n",
        "id2label = {v: k for k, v in label2id.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"nojedag/financial_phrasebank_multilingual_augmented\")\n",
        "\n",
        "def filter_by_token_length(example):\n",
        "    tokens = tokenizer.tokenize(example['sentence'])\n",
        "    return len(tokens) <= 512\n",
        "\n",
        "def prepare_dataset(dataset):\n",
        "    # dataset = dataset.filter(lambda example: filter_by_token_length(example))\n",
        "    return dataset\n",
        "\n",
        "dataset = prepare_dataset(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EDA\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def plot_class_distribution(dataset):\n",
        "    unique_labels = sorted(set(dataset['train']['labels']))\n",
        "    train_counts = [dataset['train'].filter(lambda x: x['labels'] == i).num_rows for i in unique_labels]\n",
        "    test_counts = [dataset['test'].filter(lambda x: x['labels'] == i).num_rows for i in unique_labels]\n",
        "    label_names = [id2label[i] for i in unique_labels]\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 6), sharey=True)\n",
        "    \n",
        "    # Train set\n",
        "    ax = axes[0]\n",
        "    sns.barplot(x=label_names, y=train_counts, ax=ax)\n",
        "    ax.set_title('Train Set Class Distribution')\n",
        "    ax.set_xlabel('Labels')\n",
        "    ax.set_ylabel('Counts')\n",
        "    ax.set_xticks(range(len(label_names)))\n",
        "    ax.set_xticklabels(label_names, rotation=45)\n",
        "    for i, count in enumerate(train_counts):\n",
        "        ax.text(i, count + max(train_counts)*0.01, str(count), ha='center', va='bottom', fontweight='bold')\n",
        "    \n",
        "    # Test set\n",
        "    ax = axes[1]\n",
        "    sns.barplot(x=label_names, y=test_counts, ax=ax)\n",
        "    ax.set_title('Test Set Class Distribution')\n",
        "    ax.set_xlabel('Labels')\n",
        "    ax.set_ylabel('Counts')\n",
        "    ax.set_xticks(range(len(label_names)))\n",
        "    ax.set_xticklabels(label_names, rotation=45)\n",
        "    for i, count in enumerate(test_counts):\n",
        "        ax.text(i, count + max(test_counts)*0.01, str(count), ha='center', va='bottom', fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_class_distribution(dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# let's tokenize the data for the model to be able to understand\n",
        "def tokenize_data(example):\n",
        "    return tokenizer(example['sentence'], padding='max_length', truncation=True)    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenize the dataset\n",
        "dataset = dataset.map(tokenize_data, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "# Loading a pretrain model while specifying the number of labels in our dataset for fine-tuning\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\", num_labels=3)\n",
        "# Explicitly move model to the correct device\n",
        "model = model.to(device)\n",
        "print(f\"Model moved to {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# the default batch size for training arguments\n",
        "batch_size = 16\n",
        "\n",
        "# set number of epochs\n",
        "number_of_epochs = 7\n",
        "# let set the logging steps\n",
        "logging_steps = len(dataset['train']) // batch_size # it should log each batch \n",
        "\n",
        "steps = (len(dataset['train']) / batch_size) * number_of_epochs\n",
        "warmup_steps = int(0.1 * steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import Trainer\n",
        "\n",
        "class CustomTrainer(Trainer):\n",
        "    \"\"\"\n",
        "    Custom Trainer that supports weighted loss for class imbalance and robust device handling.\n",
        "    \"\"\"\n",
        "    def __init__(self, weight_tensor, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.original_weight_tensor = weight_tensor\n",
        "        self.loss_fct = torch.nn.CrossEntropyLoss(weight=weight_tensor.to(self.model.device))\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        # Ensure the loss function's weight tensor is on the same device as the model\n",
        "        device = self.model.device\n",
        "        if hasattr(self.loss_fct, 'weight') and self.loss_fct.weight is not None:\n",
        "            if self.loss_fct.weight.device != device or self.loss_fct.weight.dtype != self.original_weight_tensor.dtype:\n",
        "                self.loss_fct = torch.nn.CrossEntropyLoss(\n",
        "                    weight=self.original_weight_tensor.to(device=device, dtype=self.original_weight_tensor.dtype)\n",
        "                )\n",
        "\n",
        "        labels = inputs.get(\"labels\")\n",
        "        if labels is None:\n",
        "            raise ValueError(\"Labels must be present in the inputs for loss computation.\")\n",
        "        # Do not pop labels, just use them\n",
        "        outputs = model(**{k: v for k, v in inputs.items() if k != \"labels\"})\n",
        "        logits = outputs.get(\"logits\") if isinstance(outputs, dict) else outputs[0]\n",
        "        labels = labels.to(device)\n",
        "        loss = self.loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "        return (loss, outputs) if return_outputs else loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from typing import Dict, Any\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "# ── metric function for Trainer ──────────────────────────────────────────────\n",
        "def compute_metrics(eval_pred) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Sentiment‑specific compute_metrics for 3‑way classification.\n",
        "    \n",
        "    Returns overall accuracy + macro P/R/F1 *and* per‑class precision/recall/F1,\n",
        "    all as flat keys so Trainer logs cleanly.\n",
        "    \"\"\"\n",
        "    # A. unpack\n",
        "    logits, labels = eval_pred\n",
        "    labels = labels.astype(int)\n",
        "\n",
        "    # B. post‑process: softmax → argmax\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "\n",
        "    # C. overall metrics\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    p_macro, r_macro, f1_macro, _ = precision_recall_fscore_support(\n",
        "        labels, preds, average=\"macro\", zero_division=0\n",
        "    )\n",
        "\n",
        "    # D. per‑class metrics\n",
        "    # precision_recall_fscore_support returns arrays ordered by the label ids we pass\n",
        "    p_cls, r_cls, f1_cls, _ = precision_recall_fscore_support(\n",
        "        labels, preds, labels=list(id2label.keys()), zero_division=0\n",
        "    )\n",
        "\n",
        "    per_class = {}\n",
        "    for idx, cls_name in id2label.items():\n",
        "        per_class[f\"{cls_name}_precision\"] = p_cls[idx]\n",
        "        per_class[f\"{cls_name}_recall\"]    = r_cls[idx]\n",
        "        per_class[f\"{cls_name}_f1\"]        = f1_cls[idx]\n",
        "\n",
        "    # E. flatten & return\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"macro_precision\": p_macro,\n",
        "        \"macro_recall\": r_macro,\n",
        "        \"macro_f1\": f1_macro,\n",
        "        **per_class,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset = dataset['train'].shuffle(seed=42) \n",
        "eval_dataset = dataset['test'].shuffle(seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=8) # pad to 8 for GPU efficiency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    num_train_epochs=number_of_epochs, \n",
        "    load_best_model_at_end=True,\n",
        "    eval_strategy='steps',  # (updated: 'eval_strategy' is deprecated)\n",
        "    save_strategy='steps',\n",
        "    learning_rate=2e-5,  \n",
        "    weight_decay=0.01,\n",
        "    logging_steps=logging_steps,\n",
        "    warmup_steps=warmup_steps,\n",
        "    save_steps=1000,\n",
        "    eval_steps=500,\n",
        "    output_dir=f\".././models/{model_name}\",\n",
        "    run_name=model_name,\n",
        "    report_to=\"wandb\",\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    gradient_accumulation_steps=2,\n",
        "    fp16=True,\n",
        "    no_cuda=False,\n",
        "    dataloader_pin_memory=True,\n",
        "    seed=42,  # Ensures reproducibility\n",
        "    dataloader_num_workers=4,  # Speeds up data loading (adjust for your CPU)\n",
        "    save_total_limit=2,  # Limits number of checkpoints to save disk space\n",
        "    metric_for_best_model=\"macro_f1\",  # Use your main metric for best model selection\n",
        "    greater_is_better=True,  # Set according to your metric\n",
        "    logging_first_step=True,  # Log the first step for early diagnostics\n",
        "    disable_tqdm=False,  # Show progress bar\n",
        "    remove_unused_columns=True,  # Speeds up training if you don't need extra columns\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Print device information for training\n",
        "print(f\"Training will use device: {'CUDA' if not training_args.no_cuda and torch.cuda.is_available() else 'CPU'}\")\n",
        "print(f\"FP16 (mixed precision): {training_args.fp16}\")\n",
        "print(f\"Batch size per device: {training_args.per_device_train_batch_size}\")\n",
        "print(f\"Gradient accumulation steps: {training_args.gradient_accumulation_steps}\")\n",
        "print(f\"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps * (torch.cuda.device_count() if torch.cuda.is_available() else 1)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "labels = dataset['train']['labels']\n",
        "counts = Counter(labels)\n",
        "total = sum(counts.values())\n",
        "weights = torch.tensor([total/counts[i] for i in range(3)], dtype=torch.float)\n",
        "weights = weights / weights.sum()  # normalize\n",
        "for labels, weight in zip(range(3), weights):\n",
        "    print(f\"Label {id2label[labels]}: {weight.item():.4f} (count: {counts[labels]})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import EarlyStoppingCallback\n",
        "\n",
        "# weights is already a tensor, just use it directly\n",
        "weight_tensor = weights.to(device)\n",
        "\n",
        "trainer = CustomTrainer(\n",
        "    model=model, args=training_args, train_dataset=train_dataset, eval_dataset=eval_dataset, \n",
        "    weight_tensor=weight_tensor, data_collator=data_collator, compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to monitor GPU memory usage\n",
        "def print_gpu_memory_stats():\n",
        "    if torch.cuda.is_available():\n",
        "        # Get the current GPU index\n",
        "        current_device = torch.cuda.current_device()\n",
        "        \n",
        "        # Reserved memory in GB\n",
        "        reserved = torch.cuda.memory_reserved(current_device) / 1e9\n",
        "        \n",
        "        # Allocated memory in GB\n",
        "        allocated = torch.cuda.memory_allocated(current_device) / 1e9\n",
        "        \n",
        "        # Max allocated memory in GB\n",
        "        max_allocated = torch.cuda.max_memory_allocated(current_device) / 1e9\n",
        "        \n",
        "        # Get total memory in GB\n",
        "        total = torch.cuda.get_device_properties(current_device).total_memory / 1e9\n",
        "        \n",
        "        print(f\"GPU Memory: Total: {total:.2f} GB | Reserved: {reserved:.2f} GB | Allocated: {allocated:.2f} GB | Max Allocated: {max_allocated:.2f} GB\")\n",
        "    else:\n",
        "        print(\"GPU is not available.\")\n",
        "\n",
        "# Print memory status before creating trainer\n",
        "print(\"GPU memory before creating trainer:\")\n",
        "print_gpu_memory_stats()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start training\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "\n",
        "# Print memory usage after training\n",
        "print(\"\\nTraining completed. GPU memory usage:\")\n",
        "print_gpu_memory_stats()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training loss\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_training_loss(trainer):\n",
        "    \"\"\"\n",
        "    Plot the training loss over time.\n",
        "    \n",
        "    Args:\n",
        "        trainer: The Trainer object containing training history.\n",
        "    \"\"\"\n",
        "    # Extract the training loss values\n",
        "    train_loss = trainer.state.log_history\n",
        "\n",
        "    # Filter for training loss entries\n",
        "    train_loss = [entry['loss'] for entry in train_loss if 'loss' in entry]\n",
        "\n",
        "    # Create an array of epochs\n",
        "    epochs = np.arange(1, len(train_loss) + 1)\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(epochs, train_loss, label='Training Loss', color='blue')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss Over Time')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "plot_training_loss(trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer_eval = Trainer(\n",
        "    model=model.to(device),  # Ensure model is on the correct device\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "print(f\"Evaluation trainer using device: {next(model.parameters()).device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer_eval.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_output = trainer_eval.predict(eval_dataset)\n",
        "logits = eval_output.predictions  # shape (N_examples, N_labels)\n",
        "labels = eval_output.label_ids     # shape (N_examples,)\n",
        "\n",
        "preds = np.argmax(logits, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "cm = confusion_matrix(labels, preds)\n",
        "\n",
        "display_labels = [\"neutral\", \"positive\", \"negative\"]  # Based on label_map in transform_labels\n",
        "disp = ConfusionMatrixDisplay(\n",
        "    confusion_matrix=cm,\n",
        "    display_labels=display_labels\n",
        ")\n",
        "fig, ax = plt.subplots(figsize=(6,6))\n",
        "disp.plot(ax=ax, cmap=\"Blues\", colorbar=False)\n",
        "plt.title(\"Confusion Matrix — SST-2 Validation\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If you want to push to the hub, uncomment this line\n",
        "model.push_to_hub(f'nojedag/{model_name}')\n",
        "trainer.push_to_hub()\n",
        "trainer_eval.push_to_hub()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
